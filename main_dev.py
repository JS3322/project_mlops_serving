# app.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Any
import onnxruntime as ort
import numpy as np
import os
import threading
import ast
from contextlib import asynccontextmanager
import uvicorn

# -------------------------------
# íŒŒë¼ë¯¸í„° ì„¤ì •
# -------------------------------
MODEL_DIR = "models"
session_cache = {}
session_lock = threading.Lock()

# -------------------------------
# ì…ë ¥ ë°ì´í„° ëª¨ë¸
# -------------------------------
class InferenceRequest(BaseModel):
    model_path: str = Field(..., description="ONNX ëª¨ë¸ ê²½ë¡œ")
    input_data: str = Field(
        default="[[0.0, 1.0, 1.0]]",
        description="2ì°¨ì› float ë°°ì—´ (Python-style string)"
    )

    def to_ndarray(self) -> np.ndarray:
        try:
            parsed = ast.literal_eval(self.input_data)
            return np.array(parsed, dtype=np.float32)
        except Exception as e:
            raise ValueError(f"ì…ë ¥ ë°ì´í„°ë¥¼ ndarrayë¡œ ë³€í™˜ ì‹¤íŒ¨: {e}")

# -------------------------------
# lifespan ì´ë²¤íŠ¸ ì²˜ë¦¬
# -------------------------------
@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸ“¦ startup: preload ONNX models from folder")
    with session_lock:
        for filename in os.listdir(MODEL_DIR):
            model_path = os.path.join(MODEL_DIR, filename)
            if not (filename.endswith(".onnx") and os.path.isfile(model_path)):
                continue
            try:
                session = ort.InferenceSession(model_path)
                session_cache[model_path] = session
                print(f"âœ… Loaded: {model_path}")
            except Exception as e:
                print(f"âŒ Failed to load {model_path}: {e}")
    yield
    print("ğŸ§¹ shutdown complete")

# -------------------------------
# FastAPI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# -------------------------------
app = FastAPI(lifespan=lifespan)

# -------------------------------
# ìºì‹œëœ ì„¸ì…˜ìœ¼ë¡œ ì¶”ë¡ 
# -------------------------------
@app.post("/infer/cached")
def infer_cached(req: InferenceRequest):
    try:
        with session_lock:
            session = session_cache.get(req.model_path)
        if session is None:
            raise HTTPException(status_code=404, detail="ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

        input_array = req.to_ndarray()
        input_name = session.get_inputs()[0].name
        output_name = session.get_outputs()[0].name
        output = session.run([output_name], {input_name: input_array})[0]
        return {"cached": True, "result": output.tolist()}

    except ValueError as ve:
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Inference failed: {e}")

# -------------------------------
# í˜¸ì¶œ ì‹œë§ˆë‹¤ ì„¸ì…˜ ìƒì„±
# -------------------------------
@app.post("/infer/uncached")
def infer_uncached(req: InferenceRequest):
    try:
        if not os.path.exists(req.model_path):
            raise HTTPException(status_code=404, detail="ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        session = ort.InferenceSession(req.model_path)
        input_array = req.to_ndarray()
        input_name = session.get_inputs()[0].name
        output_name = session.get_outputs()[0].name
        output = session.run([output_name], {input_name: input_array})[0]
        return {"cached": False, "result": output.tolist()}

    except ValueError as ve:
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Inference failed: {e}")

# -------------------------------
# í˜„ì¬ ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°„ ëª¨ë¸ ëª©ë¡
# -------------------------------
@app.get("/models", response_model=List[str])
def list_models():
    with session_lock:
        return list(session_cache.keys())

# -------------------------------
# main í•¨ìˆ˜
# -------------------------------
if __name__ == "__main__":
    uvicorn.run("main_dev:app", host="0.0.0.0", port=8000, reload=True)
